import os
import sqlite3
import numpy as np
import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.memory import ConversationBufferMemory
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


# 1. åŠ è½½å¹¶åˆ†å‰² PDF æ–‡æ¡£
def load_and_split_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()

    # åˆ†å‰²æ–‡æ¡£ä¸ºå¤šä¸ªéƒ¨åˆ†
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(pages)

    return chunks


# 2. ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆä¾‹å¦‚ Sentence-Transformersï¼‰è¿›è¡Œå‘é‡åŒ–å­˜å‚¨åˆ° SQLite
def create_vector_db(chunks, model_path, db_path="./vector_db.sqlite"):
    # åŠ è½½æœ¬åœ°æ¨¡å‹
    model = SentenceTransformer(model_path)

    # è¿æ¥ SQLite æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS embeddings (
        document_id INTEGER PRIMARY KEY,
        chunk_id INTEGER,
        embedding BLOB,
        content TEXT
    )
    """)

    # æ’å…¥åµŒå…¥ï¼ˆembeddingï¼‰æ•°æ®
    for doc_id, chunk in enumerate(chunks):
        embedding = model.encode(chunk.page_content)
        cursor.execute("""
        INSERT INTO embeddings (document_id, chunk_id, embedding, content)
        VALUES (?, ?, ?, ?)
        """, (doc_id, 0, embedding.tobytes(), chunk.page_content))

    conn.commit()
    conn.close()


# 3. æŸ¥è¯¢æ•°æ®åº“ä¸­çš„åµŒå…¥ï¼ˆembeddingï¼‰å¹¶è¿”å›æœ€ç›¸ä¼¼çš„ç»“æœ
def query_vector_db(query, model_path, db_path="./vector_db.sqlite"):
    model = SentenceTransformer(model_path)
    query_embedding = model.encode(query)

    # è¿æ¥åˆ°æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ä»æ•°æ®åº“ä¸­å–å‡ºæ‰€æœ‰åµŒå…¥æ•°æ®
    cursor.execute("SELECT chunk_id, embedding, content FROM embeddings")
    rows = cursor.fetchall()

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¹¶è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£
    similarities = []
    for row in rows:
        stored_embedding = np.frombuffer(row[1], dtype=np.float32)
        sim = cosine_similarity([query_embedding], [stored_embedding])
        similarities.append((row[2], sim[0][0]))

    # æŒ‰ç…§ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[0][0]  # è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£


# 4. åˆå§‹åŒ–é—®ç­”é“¾
def create_qa_chain():
    # ä½¿ç”¨ BM25 å’Œ SQLite å‘é‡æ•°æ®åº“åˆ›å»º EnsembleRetriever
    bm25_retriever = BM25Retriever.from_documents(chunks)
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever],
        weights=[1.0]
    )

    # å¢åŠ å†…å­˜åŠŸèƒ½ï¼Œæ”¯æŒå¤šè½®å¯¹è¯
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # è‡ªå®šä¹‰é—®ç­”é“¾
    class LocalRetrievalQA(RetrievalQA):
        def __init__(self, retriever, memory):
            super().__init__(retriever=retriever, memory=memory)

        def run(self, query):
            # æ‰§è¡Œæ£€ç´¢
            context = query_vector_db(query, model_path="./models/all-MiniLM-L6-v2")
            return context  # è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

    # æ„å»ºé—®ç­”é“¾
    qa_chain = LocalRetrievalQA(
        retriever=ensemble_retriever,
        memory=memory
    )

    return qa_chain


# 5. Streamlit ç•Œé¢
st.set_page_config(page_title="PDF æ™ºèƒ½åŠ©æ‰‹")
st.header("ğŸ“„ æœ¬åœ° PDF é—®ç­”ç³»ç»Ÿ")

# ä¸Šä¼  PDF æ–‡ä»¶
uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶", type="pdf")
if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())

    # è§¦å‘æ–‡æ¡£å¤„ç†é€»è¾‘
    chunks = load_and_split_pdf("temp.pdf")

    # è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„
    model_path = st.text_input("è¯·è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š", "./models/all-MiniLM-L6-v2")
    if model_path:
        create_vector_db(chunks, model_path)
        st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")

# æé—®ä¸å›ç­”
user_question = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
if user_question:
    response = query_vector_db(user_question, model_path="./models/all-MiniLM-L6-v2")
    st.write("å›ç­”ï¼š", response)
